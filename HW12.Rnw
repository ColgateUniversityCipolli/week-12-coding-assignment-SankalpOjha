\documentclass{article}
\usepackage[margin=1.0in]{geometry} % To set margins
\usepackage{amsmath}  % This allows me to use the align functionality.
                      % If you find yourself trying to replicate
                      % something you found online, ensure you're
                      % loading the necessary packages!
\usepackage{amsfonts} % Math font
\usepackage{fancyvrb}
\usepackage{hyperref} % For including hyperlinks
\usepackage[shortlabels]{enumitem}% For enumerated lists with labels specified
                                  % We had to run tlmgr_install("enumitem") in R
\usepackage{float}    % For telling R where to put a table/figure
\usepackage{natbib}        %For the bibliography
\bibliographystyle{apalike}%For the bibliography

\begin{document}
<<echo=F, message=F, warning=F>>=
library(tidyverse)
@

\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item A group of researchers is running an experiment over the course of 30 months, 
with a single observation collected at the end of each month. Let $X_1, ..., X_{30}$
denote the observations for each month. From prior studies, the researchers know that
\[X_i \sim f_X(x),\]
but the mean $\mu_X$ is unknown, and they wish to conduct the following test
\begin{align*}
H_0&: \mu_X = 0\\
H_a&: \mu_X > 0.
\end{align*}
At month $k$, they have accumulated data $X_1, ..., X_k$ and they have the 
$t$-statistic
\[T_k = \frac{\bar{X} - 0}{S_k/\sqrt{n}}.\]
The initial plan was to test the hypotheses after all data was collected (at the 
end of month 30), at level $\alpha=0.05$. However, conducting the experiment is 
expensive, so the researchers want to ``peek" at the data at the end of month 20 
to see if they can stop it early. That is, the researchers propose to check 
whether $t_{20}$ provides statistically discernible support for the alternative. 
If it does, they will stop the experiment early and report support for the 
researcher's alternative hypothesis. If it does not, they will continue to month 
30 and test whether $t_{30}$ provides statistically discernible support for the
alternative.

\begin{enumerate}
  \item What values of $t_{20}$ provide statistically discernible support for the
  alternative hypothesis?
<<echo=TRUE, message=FALSE, warning=FALSE>>=
library(tidyverse)
n20 <- 20
(val <- qt(0.95, df = n20-1))
@

To get a p-value which is less than 0.05 which gives us a statistically discernible support for the alternative, we need a $t_{20}$ value of greater than $1.7291$.


  \item What values of $t_{30}$ provide statistically discernible support for the
  alternative hypothesis?
<<echo=TRUE, message=FALSE, warning=FALSE>>=
n30 <- 30
(val.30 <- qt(0.95, df = n30-1))
@

To get a p-value which is less than 0.05 which gives us a statistically discernible support for the alternative, we need a $t_{30}$ value of greater than $1.6991$.


  \item Suppose $f_X(x)$ is a Laplace distribution with $a=0$ and $b=4.0$.
  Conduct a simulation study to assess the Type I error rate of this approach.\\
  \textbf{Note:} You can use the \texttt{rlaplace()} function from the \texttt{VGAM}
  package for \texttt{R} \citep{VGAM}.
  \item \textbf{Optional Challenge:} Can you find a value of $\alpha<0.05$ that yields a 
  Type I error rate of 0.05?
  
<<message = F, warning = F>>=
# part C 
# simulation study
library(VGAM)
num.sims <- 10000
a <- 0
b <- 4.0
mu0 <- 0

# errors
type1error.20.count <- 0
type1error.30.count <- 0
no.error.count <- 0

for(i in 1:num.sims){
  # sims for n = 20 peek, n = 30
  sim30.curr <- rlaplace(n=30, location = a, scale = b)
  sim20.curr <- sim30.curr[1:20]
  
  # t test for n = 20 peek
  n20.stat <- t.test(x = sim20.curr, mu = mu0, alternative = "greater")
  
  if (n20.stat$p.value < 0.05){
    type1error.20.count <- type1error.20.count + 1
  }else{
    
    n30.stat <- t.test(x = sim30.curr, mu = mu0, alternative = "greater")
    
    if (n30.stat$p.value < 0.05){
      type1error.30.count <- type1error.30.count + 1
    } else {
      no.error.count <- no.error.count + 1
    }
  }
}


(prop.20 <- type1error.20.count/num.sims)
(prop.30 <- type1error.30.count/num.sims)
(prop.no <- no.error.count/num.sims)
(tot.type1.error <- (type1error.20.count + type1error.30.count)/num.sims)
@

A type 1 error is described as when we accidentally find statistically discernible support for the alternative although the null is correct. This entails falsely rejecting the null hypothesis for the alternative, which is not true. As can be seen through the proportions, for our simulation we saw about 7.04\% type 1 error which means that we successfully made the right descion 92.96\% of the time. When checking for the type 1 error after time 20, we see a type 1 error about 4.63\% of the time and a type 1 error 2.41\% of the time for time 30. As can be seen in the percentages, the time 30 percentage is lower than the time 20 percentage meaning the decsion was more accurate. NOTE: The percentages are subject to change as each simulation is unique and the ones in the paragraph are from a particular trial.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item Perform a simulation study to assess the robustness of the $T$ test. 
  Specifically, generate samples of size $n=15$ from the Beta(10,2), Beta(2,10), 
  and Beta(10,10) distributions and conduct the following hypothesis tests against 
  the actual mean for each case (e.g., $\frac{10}{10+2}$, $\frac{2}{10+2}$, and 
  $\frac{10}{10+10}$). 
  
    <<warning = F, message = F>>=
# Problem 2
# set.seed() so results reproducible (can use xtable)
set.seed(7272)
type1.errors <- function(a, b){
  num.sims <- 1000
  alpha <- a
  beta <- b
  pop.mean <- alpha/(alpha + beta)
  samp.size <- 15
  
  # errors
  right.errors <- 0 
  left.errors <- 0
  two.errors <- 0
  
  for(i in 1:num.sims){
    # sims using rbeta, n = 15
    curr.sim <- rbeta(n=samp.size, shape1 = alpha, shape2 = beta)
    
    # t test for right tail 
    right.t <- t.test(x=curr.sim, mu = pop.mean, alternative = "greater")
    right.p <- right.t$p.value
    # t test for left tail 
    left.t <- t.test(x=curr.sim, mu = pop.mean, alternative = "less")
    left.p <- left.t$p.value
    # t test for two tailed
    two.t <- t.test(x=curr.sim, mu = pop.mean, alternative = "two.sided")
    twotail.p <- two.t$p.value
    
    # count errors
    if (right.p < 0.05){
      right.errors <- right.errors + 1
    }
    if (left.p < 0.05){
      left.errors <- left.errors + 1
    }
    if(twotail.p < 0.05){
      two.errors <- two.errors + 1
    }
  }
  
  # find proportion
  right.e <- right.errors/num.sims
  left.e <- left.errors/num.sims
  two.e <- two.errors/num.sims
  
  tibble(type = c("left tailed", "right tailed", "two-tailed"),
         errors = c(left.e, right.e, two.e)) |>
    pivot_wider(names_from = type, values_from = errors)
}

errors.data <- tibble(bind_rows(type1.errors(a = 10, b = 2),
type1.errors(a = 2, b = 10),
type1.errors(a = 10, b = 10)))

# consolidate data 
(errors.table <- tibble(distribution = c("Beta(10,2)", "Beta(2,10)", 
                                        "Beta(10,10)"), errors.data))
library(xtable)
xtable(errors.table)
@

\begin{table}[ht]
\centering
\begin{tabular}{rlrrr}
\hline
& distribution & left tailed & right tailed & two-tailed \\
\hline
1 & Beta(10,2) & 0.03 & 0.06 & 0.04 \\
2 & Beta(2,10) & 0.07 & 0.04 & 0.05 \\
3 & Beta(10,10) & 0.04 & 0.05 & 0.05 \\
\hline
\end{tabular}
\end{table}
  
  \begin{enumerate}
    \item What proportion of the time do we make an error of Type I for a
    left-tailed test?
    
    \textbf{Solution:} A Type I error for a left-tailed test is \Sexpr{errors.data$`left tailed`[1]} for a Beta(10, 2) distribution, \Sexpr{errors.data$`left tailed`[2]} for a Beta(2, 10) distribution, and \Sexpr{errors.data$`left tailed`[3]} for a Beta(10, 10) distribution.
    
    \item What proportion of the time do we make an error of Type I for a
    right-tailed test?
    
    \textbf{Solution:} A Type I error for a right-tailed test is \Sexpr{errors.data$`right tailed`[1]} for a Beta(10, 2) distribution, \Sexpr{errors.data$`right tailed`[2]} for a Beta(2, 10) distribution, and \Sexpr{errors.data$`right tailed`[3]} for a Beta(10, 10) distribution.
    \item What proportion of the time do we make an error of Type I for a
    two-tailed test?
    
    \textbf{Solution:} A Type I error for a right-tailed test is \Sexpr{errors.data$`two-tailed`[1]} for a Beta(10, 2) distribution, \Sexpr{errors.data$`two-tailed`[2]} for a Beta(2, 10) distribution, and \Sexpr{errors.data$`two-tailed`[3]} for a Beta(10, 10) distribution.
    \item How does skewness of the underlying population distribution effect
    Type I error across the test types?
    
    \textbf{Solution:} For a Beta(10, 2) distribution, which is left skewed, we can see that the proportion for right-tailed tests is higher than the proportion for left-tailed tests. For a Beta(2, 10) distribution, which is right skewed, we can see that the proportion for left-tailed tests is higher than the proportion for right-tailed tests. For a Beta(10, 10) distribution, which is symmetric, we can see that the proportion for the right-tailed, left-tailed, and two-tailed tests is about the same. 
  \end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% End Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{enumerate}
\bibliography{bibliography}
\end{document}
